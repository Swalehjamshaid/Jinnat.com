# -*- coding: utf-8 -*-
"""
FF TECH ENTERPRISE WEBSITE AUDIT SAAS
Filename: pdf_report.py (updated)
Railway-Ready | Single-File Executable | No External Dependencies (except standard libs & common py libs)
What's new in this version
--------------------------
â€¢ Expanded audit engine to populate all sections requested by user (SEO, Performance, Security, Accessibility, UX, Links, Tracking)
â€¢ Safer network timeouts, graceful fallbacks, and clear N/A markers where lab/field APIs are required
â€¢ Executive Summary with radar + bar chart, risk level, top-5 critical issues, and impact estimates
â€¢ Cover page with optional company logo, confidentiality notice, and QR to target
â€¢ Website overview (domain, IP, SSL, redirects, page size, total requests, etc.)
â€¢ Broken link sampler table with internal/external classification
â€¢ Clean, printer-friendly PDF layout
Note: This module performs best-effort static checks. Certain metrics (e.g., Core Web Vitals, FCP/LCP/TTI,
PageSpeed API results, precise hosting provider and geo-location) require external APIs or RUM/Lab tools and are
marked as N/A unless you integrate those APIs.
"""
import io
import re
import os
import ssl
import json
import time
import socket
import hashlib
import logging
import datetime as dt
from collections import Counter
from typing import Dict, Any, List, Tuple, Optional
from urllib.parse import urlparse, urljoin
import requests
from bs4 import BeautifulSoup

# PDF Rendering
from reportlab.lib import colors
from reportlab.lib.pagesizes import A4
from reportlab.lib.units import inch
from reportlab.lib.styles import getSampleStyleSheet, ParagraphStyle
from reportlab.platypus import (
    SimpleDocTemplate, Paragraph, Spacer, Table, TableStyle,
    PageBreak, Image, ListFlowable, ListItem, KeepTogether
)
from reportlab.graphics.barcode import qr
from reportlab.graphics.shapes import Drawing

# Data Visualization
import matplotlib
matplotlib.use("Agg")  # Headless-safe for server environments
import matplotlib.pyplot as plt
import numpy as np

# =========================================
# BRANDING & SCORING CONFIG
# =========================================
COMPANY = "FF Tech"
SAAS_NAME = "FF Tech Enterprise Website Audit"
VERSION = "v5.0-Enterprise"

WEIGHTS = {
    "performance": 0.30,
    "security": 0.25,
    "seo": 0.20,
    "accessibility": 0.15,
    "ux": 0.10,
}

PRIMARY_DARK = colors.HexColor("#1A2B3C")
ACCENT_BLUE = colors.HexColor("#3498DB")
SUCCESS_GREEN = colors.HexColor("#27AE60")
CRITICAL_RED = colors.HexColor("#C0392B")
WARNING_ORANGE = colors.HexColor("#F39C12")
MUTED_GREY = colors.HexColor("#7F8C8D")

USER_AGENT = 'FF-Tech-Audit-SaaS/5.0 (Railway-Node)'

# =========================================
# LOGGING
# =========================================
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger("audit")

# =========================================
# UTILITIES
# =========================================
STOPWORDS = set(
    """a an and are as at be but by for if in into is it no not of on or such that the their then there these they this to was will with you your from our""".split()
)

def safe_get(url: str, headers: Optional[dict] = None, timeout: int = 15, allow_redirects: bool = True, method: str = "GET"):
    try:
        if method.upper() == "HEAD":
            return requests.head(url, headers=headers or {"User-Agent": USER_AGENT}, timeout=timeout, allow_redirects=allow_redirects)
        elif method.upper() == "OPTIONS":
            return requests.options(url, headers=headers or {"User-Agent": USER_AGENT}, timeout=timeout, allow_redirects=allow_redirects)
        else:
            return requests.get(url, headers=headers or {"User-Agent": USER_AGENT}, timeout=timeout, allow_redirects=allow_redirects)
    except Exception as e:
        logger.debug(f"safe_get error for {url}: {e}")
        return None

def get_domain(host_or_url: str) -> str:
    parsed = urlparse(host_or_url if host_or_url.startswith("http") else f"https://{host_or_url}")
    return parsed.netloc.lower()

def get_ip(domain: str) -> Optional[str]:
    try:
        return socket.gethostbyname(domain)
    except Exception:
        return None

def get_ssl_cert_expiry(domain: str, port: int = 443) -> Optional[str]:
    """Return SSL notAfter in RFC2822-like string if available."""
    try:
        ctx = ssl.create_default_context()
        with socket.create_connection((domain, port), timeout=8) as sock:
            with ctx.wrap_socket(sock, server_hostname=domain) as ssock:
                cert = ssock.getpeercert()
                return cert.get('notAfter')
    except Exception:
        return None

def https_redirected(initial_url: str, final_url: str) -> str:
    try:
        i = urlparse(initial_url)
        f = urlparse(final_url)
        if i.scheme == "http" and f.scheme == "https":
            return "Yes"
        return "No"
    except Exception:
        return "Unknown"

def count_minified(urls: List[str]) -> Tuple[int, int]:
    total = len(urls)
    minified = sum(1 for u in urls if ".min." in u)
    return minified, total

def classify_link(base_domain: str, href: str) -> str:
    try:
        d = urlparse(href)
        if not d.netloc or d.netloc == base_domain:
            return "Internal"
        return "External"
    except Exception:
        return "Unknown"

def resolve_url(base_url: str, href: str) -> str:
    return urljoin(base_url, href)

def estimate_business_impact(issue_cat: str, severity: str) -> str:
    sev = severity.lower()
    if issue_cat.lower() in ("sec", "security"):
        if "ðŸ”´" in severity or "critical" in sev:
            return "High risk: potential data exposure, trust loss"
        if "ðŸŸ " in severity or "high" in sev:
            return "Elevated risk: security misconfiguration"
        return "Moderate risk"
    if issue_cat.lower() in ("perf", "performance"):
        return "Conversion loss due to slow UX"
    if issue_cat.lower() in ("seo",):
        return "Lower organic traffic & visibility"
    if issue_cat.lower() in ("access", "accessibility"):
        return "Exclusion of users; potential compliance risk"
    if issue_cat.lower() in ("ux",):
        return "Reduced engagement & task completion"
    return "Operational impact"

# =========================================
# CORE AUDIT ENGINE
# =========================================
class SaaSWebAuditor:
    def __init__(self, url: str):
        self.url = url if url.startswith('http') else f"https://{url}"
        self.domain = get_domain(self.url)
        self.report_id = hashlib.sha256(f"{self.url}{time.time()}".encode()).hexdigest()[:12].upper()
        self.headers = {'User-Agent': USER_AGENT}
        self.findings: List[Dict[str, str]] = []
        self.debug: Dict[str, Any] = {}

    def _log_issue(self, cat: str, severity: str, msg: str, rec: str):
        self.findings.append({"cat": cat, "sev": severity, "msg": msg, "rec": rec})

    def _status(self, ok: bool) -> str:
        return "PASS" if ok else "FAIL"

    def _yesno(self, ok: Optional[bool]) -> str:
        if ok is True:
            return "Yes"
        if ok is False:
            return "No"
        return "Unknown"

    def run_audit(self) -> Dict[str, Any]:
        try:
            t0 = time.time()
            res = safe_get(self.url, headers=self.headers, timeout=20, allow_redirects=True)
            if res is None:
                raise RuntimeError("Primary request failed")
            t_fetch_ms = (time.time() - t0) * 1000
            soup = BeautifulSoup(res.text or '', 'html.parser')
            final_url = res.url

            # ================= Performance =================
            p_score = 100
            if t_fetch_ms > 2000:
                p_score -= 25
                self._log_issue("Perf", "ðŸ”´ Critical", "Slow First Byte/Fetch time (>2s)", "Enable CDN/edge caching, tune server, optimize DB.")
            size_bytes = len(res.content or b'')
            if size_bytes > 2.5 * 1024 * 1024:  # 2.5 MB
                p_score -= 15
                self._log_issue("Perf", "ðŸŸ  High", "Heavy Page Weight (>2.5MB)", "Compress images, minify JS/CSS, remove unused assets.")
            scripts = [resolve_url(final_url, s.get('src', '')) for s in soup.find_all('script') if s.get('src')]
            styles = [resolve_url(final_url, l.get('href', '')) for l in soup.find_all('link', rel=lambda v: v and 'stylesheet' in v) if l.get('href')]
            images = [resolve_url(final_url, i.get('src', '')) for i in soup.find_all('img') if i.get('src')]
            approx_requests = 1 + len(scripts) + len(styles) + len(images)
            if approx_requests > 80:
                p_score -= 10
                self._log_issue("Perf", "ðŸŸ¡ Medium", "High number of resource requests", "Bundle & defer assets; enable HTTP/2 or HTTP/3.")
            enc = (res.headers.get('Content-Encoding') or '').lower()
            compression_enabled = enc in ("gzip", "br", "deflate")
            if not compression_enabled:
                p_score -= 5
                self._log_issue("Perf", "ðŸŸ¡ Medium", "Compression not enabled on HTML", "Enable GZIP/Brotli at CDN/server.")
            cache_control = res.headers.get('Cache-Control', '')
            has_caching = any(k in cache_control.lower() for k in ["max-age", "s-maxage", "public"])
            if not has_caching:
                p_score -= 5
                self._log_issue("Perf", "ðŸŸ¡ Medium", "No explicit caching headers", "Add Cache-Control/ETag for static assets.")
            min_js, total_js = count_minified(scripts)
            min_css, total_css = count_minified(styles)
            modern_images = sum(1 for u in images if any(u.lower().endswith(ext) for ext in [".webp", ".avif"]))
            img_opt_ok = modern_images >= max(1, int(0.3 * len(images))) if images else True
            if not img_opt_ok:
                p_score -= 5
                self._log_issue("Perf", "ðŸŸ¡ Medium", "Images not using modern formats", "Prefer WebP/AVIF; enable responsive images & lazy loading.")
            lazy_count = sum(1 for i in soup.find_all('img') if (i.get('loading') or '').lower() == 'lazy')
            lazy_ok = lazy_count >= max(1, int(0.3 * len(images))) if images else True

            # ================= SEO =================
            s_score = 100
            title_tag = soup.title.string.strip() if soup.title and soup.title.string else ''
            if not title_tag:
                s_score -= 30
                self._log_issue("SEO", "ðŸ”´ Critical", "Missing Title Tag", "Add a unique, keyword-optimized <title> (~55-60 chars).")
            else:
                if len(title_tag) < 20 or len(title_tag) > 65:
                    s_score -= 5
                    self._log_issue("SEO", "ðŸŸ¡ Medium", "Title length suboptimal", "Target 50â€“60 characters, include primary keyword.")
            meta_desc_el = soup.find("meta", attrs={"name": "description"})
            meta_desc = meta_desc_el.get('content', '').strip() if meta_desc_el else ''
            if not meta_desc:
                s_score -= 20
                self._log_issue("SEO", "ðŸŸ  High", "Missing Meta Description", "Write a compelling 150â€“160 character summary.")
            else:
                if len(meta_desc) < 100 or len(meta_desc) > 180:
                    s_score -= 5
                    self._log_issue("SEO", "ðŸŸ¡ Medium", "Meta description length suboptimal", "Keep between 150â€“160 characters.")
            h1s = soup.find_all('h1')
            h2s = soup.find_all('h2')
            if len(h1s) == 0:
                s_score -= 10
                self._log_issue("SEO", "ðŸŸ  High", "Missing H1 heading", "Add one clear H1 per page with primary keyword.")
            canonical = soup.find('link', rel=lambda v: v and 'canonical' in v.lower())
            has_canonical = canonical is not None
            robots_meta = soup.find('meta', attrs={'name': 'robots'})
            robots_meta_content = (robots_meta.get('content') or '').lower() if robots_meta else ''
            indexable = ('noindex' not in robots_meta_content)
            robots_txt_url = f"{urlparse(final_url).scheme}://{self.domain}/robots.txt"
            robots_txt_res = safe_get(robots_txt_url, timeout=8)
            robots_txt_ok = (robots_txt_res is not None and robots_txt_res.status_code == 200)
            sitemap_present = False
            sitemap_url = f"{urlparse(final_url).scheme}://{self.domain}/sitemap.xml"
            if robots_txt_ok:
                try:
                    if 'sitemap' in (robots_txt_res.text or '').lower():
                        sitemap_present = True
                except Exception:
                    pass
            sm_res = safe_get(sitemap_url, timeout=8)
            if sm_res is not None and sm_res.status_code == 200:
                sitemap_present = True
            og_tags = soup.find_all('meta', attrs={'property': re.compile(r'^og:', re.I)})
            tw_tags = soup.find_all('meta', attrs={'name': re.compile(r'^twitter:', re.I)})
            imgs = soup.find_all('img')
            alt_missing = sum(1 for i in imgs if not i.get('alt'))
            anchors = [a for a in soup.find_all('a') if a.get('href')]
            absolute_links = [(a.get_text(strip=True)[:60], resolve_url(final_url, a['href'])) for a in anchors]
            sampled = absolute_links[:40]
            link_table = []
            for anchor_text, href in sampled:
                typ = classify_link(self.domain, href)
                r = safe_get(href, timeout=6, allow_redirects=True)
                status = r.status_code if (r is not None and hasattr(r, 'status_code')) else 'ERR'
                link_table.append({"url": href, "status": status, "anchor": anchor_text, "type": typ})
            broken_internal = [l for l in link_table if l['type'] == 'Internal' and isinstance(l['status'], int) and l['status'] >= 400]
            if broken_internal:
                s_score -= 10
                self._log_issue("SEO", "ðŸŸ  High", f"Broken internal links detected ({len(broken_internal)})", "Fix or remove broken links; set proper 301s.")
            mobile_meta = soup.find('meta', attrs={'name': 'viewport'}) is not None
            structured_data = soup.find('script', attrs={'type': 'application/ld+json'}) is not None
            redirect_chain_note = "Sampled via requests; deep crawl recommended"

            # ================= Security =================
            sec_score = 100
            headers_lower = {k.lower(): v for k, v in res.headers.items()}
            ssl_ok = final_url.startswith("https")
            if not ssl_ok:
                sec_score -= 50
                self._log_issue("Sec", "ðŸ”´ Critical", "Insecure Connection (HTTP)", "Install TLS certificate and force HTTPS.")
            csp_missing = 'content-security-policy' not in headers_lower
            if csp_missing:
                sec_score -= 15
                self._log_issue("Sec", "ðŸŸ¡ Medium", "Missing Content-Security-Policy", "Set CSP to mitigate XSS/data injection.")
            hsts = 'strict-transport-security' in headers_lower
            xfo = 'x-frame-options' in headers_lower
            xxss = 'x-xss-protection' in headers_lower
            xcto = 'x-content-type-options' in headers_lower
            mixed_issues = []
            if final_url.startswith('https'):
                http_resources = [u for u in scripts + styles + images if u.startswith('http://')]
                if http_resources:
                    sec_score -= 10
                    mixed_issues = http_resources[:10]
                    self._log_issue("Sec", "ðŸŸ  High", "Mixed content (HTTP assets on HTTPS page)", "Serve all assets over HTTPS.")
            admin_paths = ["/admin", "/wp-admin", "/login", "/user/login"]
            exposed_admin = []
            base_root = f"{urlparse(final_url).scheme}://{self.domain}"
            for p in admin_paths:
                r = safe_get(base_root + p, timeout=5)
                if r is not None and r.status_code in (200, 401, 403):
                    exposed_admin.append(base_root + p)
            if exposed_admin:
                sec_score -= 5
                self._log_issue("Sec", "ðŸŸ¡ Medium", "Admin endpoints exposed", "Restrict access (IP allowlist, auth, obscure paths).")
            opt = safe_get(base_root, timeout=6, method="OPTIONS")
            allow_hdr = (opt.headers.get('Allow') if opt else '') if hasattr(opt, 'headers') else ''
            set_cookies = res.headers.get('Set-Cookie', '')
            insecure_cookies = []
            for chunk in set_cookies.split(','):
                if '=' in chunk:
                    if 'secure' not in chunk.lower() or 'httponly' not in chunk.lower():
                        insecure_cookies.append(chunk.strip().split(';')[0])
            if insecure_cookies:
                sec_score -= 5
                self._log_issue("Sec", "ðŸŸ¡ Medium", "Cookies missing Secure/HttpOnly", "Mark cookies as Secure & HttpOnly; set SameSite.")
            ssl_expiry = get_ssl_cert_expiry(self.domain) if final_url.startswith('https') else None

            # ================= Accessibility =================
            a11y_score = 100
            if alt_missing > 0:
                a11y_score -= min(20, alt_missing)
                self._log_issue("Access", "ðŸŸ¡ Medium", f"{alt_missing} images missing ALT", "Provide descriptive alt text for images.")
            inputs = soup.find_all('input')
            labels = soup.find_all('label')
            label_for = {l.get('for') for l in labels if l.get('for')}
            unlabeled_inputs = [i for i in inputs if (i.get('id') and i.get('id') not in label_for)]
            if unlabeled_inputs:
                a11y_score -= 10
                self._log_issue("Access", "ðŸŸ¡ Medium", "Form inputs without labels", "Associate <label for> with input id; use ARIA when needed.")
            aria_present = any(attr for attr in soup.find_all(attrs={"aria-label": True}))
            semantic_ok = any(soup.find(tag) for tag in ["header", "nav", "main", "article", "footer"]) or False
            if a11y_score >= 90:
                a11y_level = "WCAG A (est.)"
            elif a11y_score >= 75:
                a11y_level = "WCAG AA (est.)"
            else:
                a11y_level = "Below AA (est.)"

            # ================= UX =================
            ux_score = 100
            if not mobile_meta:
                ux_score -= 15
                self._log_issue("UX", "ðŸŸ  High", "Missing viewport meta for mobile", "Add <meta name=\"viewport\" content=\"width=device-width, initial-scale=1\">.")
            cta_texts = ["buy", "get started", "contact", "signup", "sign up", "try", "demo", "start"]
            ctas = [a for a in anchors if any(t in (a.get_text(" ").strip().lower()) for t in cta_texts)]
            if not ctas:
                ux_score -= 5
                self._log_issue("UX", "ðŸŸ¡ Medium", "No prominent call-to-action detected", "Add clear CTA buttons above the fold.")
            broken_buttons = [a for a in anchors if a.get('href') in ('#', 'javascript:void(0)')]
            if broken_buttons:
                ux_score -= 5
                self._log_issue("UX", "ðŸŸ¡ Medium", "Links/buttons without destination", "Provide valid destinations or click handlers.")

            # ================= CMS / Tracking =================
            generator_meta = soup.find('meta', attrs={'name': 'generator'})
            generator_val = (generator_meta.get('content') or '').lower() if generator_meta else ''
            cms = "Custom/Unknown"
            html_text = res.text.lower() if res.text else ''
            if "wp-content" in html_text or "wordpress" in generator_val:
                cms = "WordPress"
            elif "shopify" in html_text or "x-shopify-stage" in ''.join(res.headers.keys()).lower():
                cms = "Shopify"
            elif "wix" in html_text:
                cms = "Wix (heuristic)"
            tracking = {
                "ga4": bool(re.search(r"G-[-A-Z0-9]{6,}", res.text or '', re.I)) or ("gtag/js" in html_text),
                "ua": bool(re.search(r"UA-\\d{4,}-\\d+", res.text or '', re.I)),
                "gtm": bool(re.search(r"GTM-[A-Z0-9]{5,}", res.text or '', re.I)),
                "fb_pixel": "fbq(" in html_text or "connect.facebook.net/en_US/fbevents.js" in html_text,
                "conversion": any(k in html_text for k in ["conversion", "purchase", "gtm.ecommerce"]),
            }

            # ================= Scoring Aggregate =================
            scores = {
                "performance": max(0, min(100, p_score)),
                "seo": max(0, min(100, s_score)),
                "security": max(0, min(100, sec_score)),
                "accessibility": max(0, min(100, a11y_score)),
                "ux": max(0, min(100, ux_score)),
            }
            overall = sum(scores[k] * WEIGHTS[k] for k in WEIGHTS)

            if overall >= 85:
                risk_level = "Low"
            elif overall >= 70:
                risk_level = "Medium"
            elif overall >= 50:
                risk_level = "High"
            else:
                risk_level = "Critical"

            critical_sorted = sorted(self.findings, key=lambda x: (0 if 'ðŸ”´' in x['sev'] else 1, 0 if 'ðŸŸ ' in x['sev'] else 1))
            top5 = critical_sorted[:5]
            impact_notes = [
                {**iss, "impact": estimate_business_impact(iss['cat'], iss['sev'])}
                for iss in top5
            ]

            ip = get_ip(self.domain)
            ssl_status = f"Valid (expiry: {ssl_expiry})" if (ssl_expiry is not None) else ("N/A" if not ssl_ok else "Unknown validity")
            redirect_http_to_https = https_redirected(self.url, final_url)
            overview = {
                "domain": self.domain,
                "ip": ip or "Unknown",
                "hosting_provider": "N/A (not detected)",
                "server_location": "N/A (requires geo-IP API)",
                "cms": cms,
                "ssl_status": ssl_status,
                "http_to_https": redirect_http_to_https,
                "page_load_time_ms": int(t_fetch_ms),
                "page_size_kb": round(size_bytes / 1024.0, 1),
                "total_requests_approx": approx_requests,
            }

            mixed_sample = mixed_issues

            a11y_details = {
                "missing_alt": alt_missing,
                "contrast_issues": "N/A (requires color analysis)",
                "missing_aria_labels": "Heuristic: %s" % ("Some present" if aria_present else "Few/None detected"),
                "form_label_issues": len(unlabeled_inputs),
                "keyboard_support": "N/A (requires interaction test)",
                "semantic_html": "Good" if semantic_ok else "Needs improvement",
                "compliance_level": a11y_level,
            }

            perf_details = {
                "fcp": "N/A (lab tool required)",
                "lcp": "N/A (lab tool required)",
                "tti": "N/A (lab tool required)",
                "total_blocking_time": "N/A (lab tool required)",
                "pagespeed": "N/A (API not integrated)",
                "compression": self._yesno(compression_enabled),
                "caching": self._yesno(has_caching),
                "minified_js": f"{min_js}/{total_js}",
                "minified_css": f"{min_css}/{total_css}",
                "image_optimization": self._yesno(img_opt_ok),
                "lazy_loading": self._yesno(lazy_ok),
            }

            seo_details = {
                "title": title_tag,
                "title_length": len(title_tag) if title_tag else 0,
                "meta_description_length": len(meta_desc) if meta_desc else 0,
                "h1_count": len(h1s),
                "h2_count": len(h2s),
                "canonical_present": self._yesno(has_canonical),
                "robots_txt": self._yesno(robots_txt_ok),
                "sitemap_xml": self._yesno(sitemap_present),
                "open_graph": self._yesno(len(og_tags) > 0),
                "twitter_cards": self._yesno(len(tw_tags) > 0),
                "images_missing_alt": alt_missing,
                "broken_internal_links": len(broken_internal),
                "indexable": self._yesno(indexable),
                "mobile_responsive": self._yesno(mobile_meta),
                "core_web_vitals": "N/A (API not integrated)",
                "structured_data": self._yesno(structured_data),
                "redirect_chains": redirect_chain_note,
                "duplicate_content": "N/A (requires crawl)"
            }

            sec_details = {
                "ssl_certificate_validity": ssl_status,
                "hsts_enabled": self._yesno(hsts),
                "security_headers": {
                    "Content-Security-Policy": self._yesno(not csp_missing),  # FIXED LINE
                    "X-Frame-Options": self._yesno(xfo),
                    "X-XSS-Protection": self._yesno(xxss),
                    "X-Content-Type-Options": self._yesno(xcto),
                },
                "mixed_content_sample": mixed_sample,
                "exposed_admin_panels": exposed_admin[:5],
                "http_methods_allowed": allow_hdr or "Unknown",
                "cookies_secure_httponly": self._yesno(len(insecure_cookies) == 0),
            }

            ux_details = {
                "mobile_friendly": self._yesno(mobile_meta),
                "viewport_configured": self._yesno(mobile_meta),
                "navigation_clarity": "Heuristic: nav present" if soup.find('nav') else "Needs review",
                "cta_visibility": "Heuristic: present" if ctas else "Low",
                "popup_intrusiveness": "N/A (dynamic behavior)",
                "broken_buttons": len(broken_buttons),
                "form_usability": "Heuristic: labels missing" if unlabeled_inputs else "Good",
            }

            broken_table = [l for l in link_table if isinstance(l['status'], int) and l['status'] >= 400]

            tracking_details = {
                "ga4_detected": self._yesno(tracking["ga4"]),
                "ua_detected": self._yesno(tracking["ua"]),
                "gtm_detected": self._yesno(tracking["gtm"]),
                "facebook_pixel": self._yesno(tracking["fb_pixel"]),
                "conversion_tracking": self._yesno(tracking["conversion"]),
                "missing_tracking": "Review if none detected",
            }

            data = {
                "meta": {
                    "url": self.url,
                    "final_url": final_url,
                    "domain": self.domain,
                    "id": self.report_id,
                    "timestamp": dt.datetime.now().strftime("%Y-%m-%d %H:%M"),
                    "overall": round(overall, 1),
                    "risk": risk_level,
                },
                "scores": scores,
                "issues": self.findings,
                "top_issues": impact_notes,
                "overview": overview,
                "seo": seo_details,
                "performance": perf_details,
                "security": sec_details,
                "accessibility": a11y_details,
                "ux": ux_details,
                "broken_links": broken_table,
                "tracking": tracking_details,
            }
            return data
        except Exception as e:
            return {"error": str(e)}

# =========================================
# PDF & VISUALIZATION GENERATOR
# =========================================
class ReportGenerator:
    def __init__(self, data: Dict[str, Any], logo_path: Optional[str] = None):
        self.data = data
        self.logo_path = logo_path
        self.styles = getSampleStyleSheet()
        self.styles.add(ParagraphStyle('SmallMuted', fontSize=8, textColor=MUTED_GREY))
        self.styles.add(ParagraphStyle('H2', parent=self.styles['Heading2'], textColor=PRIMARY_DARK))
        self.styles.add(ParagraphStyle('H3', parent=self.styles['Heading3'], textColor=PRIMARY_DARK))

    def _radar_chart(self) -> io.BytesIO:
        labels = [k.upper() for k in WEIGHTS.keys()]
        stats = [self.data['scores'][k.lower()] for k in labels]
        angles = np.linspace(0, 2*np.pi, len(labels), endpoint=False).tolist()
        stats += stats[:1]
        angles += angles[:1]
        fig, ax = plt.subplots(figsize=(4.8, 4.8), subplot_kw=dict(polar=True))
        ax.fill(angles, stats, color='#3498DB', alpha=0.25)
        ax.plot(angles, stats, color='#2980B9', linewidth=2)
        ax.set_yticklabels([])
        ax.set_xticks(angles[:-1])
        ax.set_xticklabels(labels, fontsize=9, fontweight='bold')
        fig.tight_layout()
        buf = io.BytesIO()
        plt.savefig(buf, format='png', transparent=True, dpi=140)
        plt.close(fig)
        buf.seek(0)
        return buf

    def _bar_chart(self) -> io.BytesIO:
        cats = list(self.data['scores'].keys())
        vals = [self.data['scores'][c] for c in cats]
        fig, ax = plt.subplots(figsize=(5.6, 3.2))
        bars = ax.bar([c.upper() for c in cats], vals, color=['#2E86C1', '#1ABC9C', '#C0392B', '#8E44AD', '#F39C12'])
        ax.set_ylim(0, 100)
        ax.set_ylabel('Score')
        for b, v in zip(bars, vals):
            ax.text(b.get_x()+b.get_width()/2, v + 1, f"{v}", ha='center', fontsize=8)
        fig.tight_layout()
        buf = io.BytesIO()
        plt.savefig(buf, format='png', transparent=True, dpi=140)
        plt.close(fig)
        buf.seek(0)
        return buf

    # The rest of the ReportGenerator class (cover page, sections, footer, generate_pdf) remains unchanged
    # ... (omitted for brevity in this message, but you already have it correct in your file)

# =========================================
# MAIN EXECUTION (Example)
# =========================================
if __name__ == "__main__":
    TARGET = os.environ.get("AUDIT_TARGET", "https://www.google.com")
    LOGO = os.environ.get("AUDIT_LOGO", None)  # optional
    print(f"[*] Initializing Enterprise Audit for: {TARGET}")
    auditor = SaaSWebAuditor(TARGET)
    audit_data = auditor.run_audit()
    if "error" not in audit_data:
        report = ReportGenerator(audit_data, logo_path=LOGO)
        out_file = os.environ.get("AUDIT_OUT", "Enterprise_Audit_Report.pdf")
        report.generate_pdf(out_file)
        print(f"[+] Success! Report saved as: {out_file}")
    else:
        print(f"[!] Error during audit: {audit_data['error']}")
